## 信息熵

上篇文介绍了信息量的概念，针对一个已经发生了的随机事件，它传递给我们一定的信息量。那如果是一个还没有发生的随机事件，我们该怎么度量它可能带给我们的信息量呢？度量一个随机变量的信息量就用到这篇文章将要介绍的信息熵概念。

大部分人接触熵的概念是通过物理学，物理学上用熵来量化一个系统状态的混乱程度。比如由热力学第二定律知宇宙的熵值总是增加的，所以热力学第二定律也叫熵增定律。信息熵和物理熵本质没什么不同。


**路德维希·爱德华·玻尔兹曼**（德语：**Ludwig Eduard Boltzmann** ，1844年2月20日－1906年9月5日），奥地利物理学家和哲学家。墓碑上镌刻着熵公式，k是玻尔兹曼常数。


**克劳德·艾尔伍德·香农**（英语：**Claude Elwood Shannon**，1916年4月30日－2001年2月26日），美国数学家、电子工程师和密码学家，信息论的创始人。将热力学熵引入信息论，信息熵又叫香农熵。


先说什么是随机变量：
> 随机变量X是一个**函数**，其将一个样本空间S（样本空间是一个随机事件所有可能取值的集合），映射到一个数集Y。如扔一次硬币是一个随机变量：X（实际上仍一次硬币意味着在样本空间中取一个值），其样本空间为S：{1(正面)，0(反面)}两个值的集合，X将S映射到Y：{1(出现正面)，0(出现反面)}

离散随机变量：
> 值域的元素为有限多个或可数无限多个的随机变量，可数的概念以后单独写篇文章介绍。

再介绍下数学期望：
> 考虑一枚理想的硬币，其出现正面和反面的机会都是1/2，如果我们扔了特别特别多次，会发现出现正面和反面的次数是一半一半，出现正面我们记为1，反面记为0，我们统计其平均数会发现等于0.5。
> 这个0.5的值和你扔多少次硬币没有关系，它是硬币本身的属性，只依赖于硬币的正反面概率，我们给它起了个名字，叫它**数学期望**。

再举个栗子，假如我们有一枚不均匀的硬币，其出现正面的概率为3/4，出现反面的概率为1/4，我们扔了特别特别多次，假设总次数为N，会发现出现正面1的次数为出现反面0的次数的3倍，正反面次数分别为$\frac 34N$和$\frac 14N$，我们统计其平均数：
$$\frac{\frac 34N*1 + \frac 14N*0}N=\frac 34*1 + \frac 14*0$$
发现了没，这个平均数（我们起名叫**数学期望**）确实和N无关，只与硬币本身的概率属性还有我们对正反面的编码（{正面:1，反面:0}）有关。

很自然的就能给出离散随机变量的数学期望公式了：**设离散随机变量X的值域为$\{x_1,x_2,...x_i,...x_n\}$，其$取值为x_i$的概率为$p_{x_i}$，则其数学期望为：**
$$E(X) = \sum\limits_{i=1}^n p_{x_i}x_i $$

数学期望表示的是随机变量值域取值的平均值。

***

信息熵就是一个**随机变量其值域用信息量编码后的数学期望。**

继续举栗，我们求抛一次上面那枚不均匀硬币这个随机变量X的信息熵：
1. 首先看其值域：$\{1(出现正面), 0(出现正面)\}$
2. 然后对值域用信息量进行编码：$\{I[1(出现正面)], I[0(出现正面)]\}$，求出来也就是$\{log_2\frac 43,\  2\}(单位是bit)$，约为$\{0.415, \ 2\}(单位是bit)$， 不会求的同学请看上篇。
3. 最后信息熵就等于信息量的数学期望：$H(X)=E[I(X)] = \frac 34\times log_2\frac 43 + \frac 14\times2 \approx 0.811 \ bit$ 

最后给出求离散随机变量信息熵的公式：**设离散随机变量X的值域为$\{x_1,x_2,...x_i,...x_n\}$，其$取值为x_i$的概率为$p_{x_i}$，则其信息熵为：**
$$H(X)=\sum\limits_{i=1}^n p_{x_i}I(x_i)=\sum\limits_{i=1}^n p_{x_i}log_b \frac 1{p_{x_i}}$$

式中的b是对数的底，b的取值决定信息熵的单位，如b取2信息量的单位是bit，b取自然常数e信息量的单位是nat，容易计算$1\ nat = log_2e\approx1.443\ bit$。

数学期望表示的是随机变量值域取值的平均值，信息熵既然是随机变量值域用信息量编码后的数学期望，它表示的自然就是随机变量值域取值所示信息量的平均值。如使用汉字写作的信息熵为H，则一本厚厚的中文书中平均每个汉字所传达的信息量就是H。


---

最后从直觉上理解一下信息熵的概念：**信息熵就是一个随机变量的不确定性程度。**

比如我们随机抛一个硬币，抛掷结果我们是不确定的，但是究竟有多么不确定呢？信息熵就是衡量的这个不确定性的。
直觉上，当硬币是均匀的抛掷结果应该是最不确定的，若硬币极端不均匀，比如只有正面，那抛掷结果就是确定的，信息熵为0。

最后附上一张抛掷一次不同均匀程度的硬币的信息熵图（单位是bit）：



但是要注意信息熵和概率的关系并不是线性的，比如扔一个均匀骰子的信息熵并不等于扔一个均匀硬币信息熵的三倍。


玻尔兹曼
